{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heuristic Clustering Algorithm\n",
    "##### print command are left as it is so it can be tested.\n",
    "##### indent has to be carefully managed."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This block imports the necessary library for computation and operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "#import matplotlib\n",
    "import math\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Make the graphs a bit prettier, and bigger\n",
    "#pd.set_option('display.width', 5000) \n",
    "#pd.set_option('display.max_columns', 60) \n",
    "\n",
    "numberOfCharacterAttributes=3\n",
    "numberOfnumericalAttributes=6\n",
    "numberOfAttributes=numberOfCharacterAttributes+numberOfnumericalAttributes\n",
    "numberOfSamples=5\n",
    " #This represents the ip address of the dataset ,or any other heavystring data preventing truncation\n",
    "characterDataType=\"<U32\"\n",
    "#This is the number of rows imported from csv file.\n",
    "DATASET_SIZE=20 \n",
    "#This is the name of dataset that is imported derive the file path correctly if file has be changed\n",
    "name_OF_FILE='dataset' \n",
    "generalizedData=0\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "why characterDataType is important?\n",
    "Ans: so that we preserve the data in the array,conversion from one datatype to another while processing with array \n",
    "may lead to truncation of data.So to overcome this issue we defined characterDataType.\n",
    "\n",
    "If in-case data is being truncated increase the number portion from the dataType:\n",
    "eg:<U21,<U30 ...etc which ever likely to be fit."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This below code reads the csv file from the location provided.\n",
    "Only few data row has been selected currently.Please load complete file while it is final.\n",
    "Determining DATA_SIZE may change the structure of cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#All dataset Parameters\n",
    "dataset=pd.read_csv(name_OF_FILE)\n",
    "dataset=dataset.drop(['No.'],axis=1)\n",
    "dataset=np.array(dataset)\n",
    "\n",
    "#comment the below line while computing on complete data\n",
    "#for performance measure only selected 100 of records\n",
    "dataset=dataset[np.random.randint(dataset.shape[0],size=100),:]\n",
    "sampleData=dataset[np.random.randint(dataset.shape[0], size=DATASET_SIZE), :]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Why 'No.' is dropped from dataset?\n",
    "Ans: It doesnot provide any meaning to the data.It is just a column of unique elements."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This below code divides the a single row to character and numerical attributes.\n",
    "For our data,which consists of 9 column , first 3 column is character attributes and else are numerical attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 871,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#data is an array of the attributes known as 'e'.\n",
    "#a single data row\n",
    "def numericalAndCharacterDivision(data):\n",
    "    '''\n",
    "\n",
    "    This function divides a single row of data set to numerical and character attributes\n",
    "    Parameter:a single row of data\n",
    "    Returns:Object of numerical attributes ,character attributes\n",
    "    \n",
    "    '''\n",
    "    #get the single row of data\n",
    "    #divide into two different group\n",
    "    division=np.split(data,[numberOfCharacterAttributes])# 3 is used as in the set of attributes first 3 data\n",
    "    #are of character attributes and rest of them are numerical attributes\n",
    "    #return as object composed of numerical and character attributes    \n",
    "    return (division[0],division[1])\n",
    "\n",
    "#numericalAndCharacterDivision(np.array([1,2,3,4,5,6,7,8,9]))\n",
    "#returns (array([1, 2, 3]), array([4, 5, 6, 7, 8, 9]))\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This below code summarizes as:\n",
    "\n",
    "e consist of attributes such as \n",
    "e:source IP Address,Destination IP Address,Protocol,Source Port,Destination Port,Sequence number,\n",
    "    Acknowledge number,length,window size\n",
    "\n",
    "\n",
    "Parameter (data):A data row consisting of both the numerical and character attributes.\n",
    "returns:P where P=Pn + Ps ; \n",
    "where Pn=centre of numerical attributes,Ps=centre of character attributes\n",
    "\n",
    "This function computes the cluster centre\n",
    "data may be object or list of all the attributes (both numerical and character attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 872,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result:  ['71.126.222.64' 'b' 'c' '19091' '30' '3' '1' '20' '1949']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def count(data):\n",
    "    \n",
    "    '''\n",
    "    this function computes data as list\n",
    "    '''\n",
    "    \n",
    "    #retrieving numerical and character attributes from data\n",
    "    #numerical,character=data\n",
    "    #numerical=np.array(numerical)\n",
    "    shapeOfData=np.array(data).shape\n",
    "    numberOfRows=shapeOfData[0]\n",
    "    numberOfCol=shapeOfData[1]\n",
    "    \n",
    "    #initializing the character and numerical lists based on the dataset\n",
    "    numerical=np.zeros([numberOfRows,numberOfnumericalAttributes])\n",
    "    character=np.zeros([numberOfRows,numberOfCharacterAttributes],dtype=characterDataType)\n",
    "    \n",
    "    #insert the data based on the attributes so that calculation would be easy\n",
    "    for i in range(np.array(data).shape[0]):\n",
    "        characterAttributes,numericalAttributes=numericalAndCharacterDivision(np.array(data)[i])\n",
    "        #print(\"n: \",numericalAttributes)\n",
    "        numerical[i]=numericalAttributes\n",
    "        character[i]=characterAttributes\n",
    "        #print(\"C: \",characterAttributes.dtype)\n",
    "\n",
    "    \n",
    "    #Preprocessing, which replaces every 'nan' item from the data to '0'\n",
    "    j=0\n",
    "    for i in numerical:\n",
    "        #print(numerical.shape)\n",
    "        for k in range(numerical.shape[1]):            \n",
    "            #print(numerical[j][k])\n",
    "            if(numerical[j][k].astype('str') == 'nan'):\n",
    "                numerical[j][k]=generalizedData              \n",
    "        j=j+1\n",
    "    #print(\"new Character: \",character)\n",
    "    \n",
    "    #numerical data as:[[2,3,4,5],[5,6,7,8]]\n",
    "    #For Pn:    \n",
    "    #its sum should be as 2+5=7,3+6=9,4+7=11,5+8=13\n",
    "    #and divide the sum by the total number of items\n",
    "    \n",
    "    #print(\"numerical: \",numerical.astype('int'))\n",
    "    numerical=numerical.astype('int')\n",
    "    numericalSum=numerical.sum(axis=0)\n",
    "    numericalRowSize=numerical.shape[0]\n",
    "\n",
    "    #print(\"Column Size: \",numericalRowSize)\n",
    "    #print(\"Sum: \",numericalSum)\n",
    "    \n",
    "    Pn=(numericalSum/numericalRowSize).astype('int')\n",
    "    #print(\"Pn: \",Pn)\n",
    "    #print(\"Sum: \",numericalSum)\n",
    "    #print(\"Row: \",numericalRowSize)\n",
    "   \n",
    "    #For Ps\n",
    "    #Ps is the frequent character attribute set which consist of q(q=m-p) most frequent character attribute\n",
    "     \n",
    "    Ps=np.empty([numberOfCharacterAttributes],dtype=characterDataType)\n",
    "    \n",
    "    #print(\"Character: \",character)\n",
    "    \n",
    "    splittedColumn=np.hsplit(character,numberOfCharacterAttributes)\n",
    "    \n",
    "    j=0\n",
    "    \n",
    "    #the splitted column is flattened that merges the data to be in same list\n",
    "    #most common determines the frequency of element in the list\n",
    "    #most_common(1) returns the first element from the count received sorted in descending order\n",
    "    #Ps is provided with single data fitted to be made as a single row 'e'\n",
    "    for i in splittedColumn:\n",
    "        #print(\"Pre: \",i)\n",
    "        Ps[j]=Counter(i.flatten()).most_common(1)[0][0]\n",
    "        j=j+1\n",
    "    \n",
    "    #print(\"Ps: \",Ps)\n",
    "    \n",
    "    \n",
    "    #All the datarow can be set as character set to prevent the loss of information from the character set.\n",
    "    P=np.empty(numberOfAttributes).astype(characterDataType)\n",
    "    j=0\n",
    "    \n",
    "    #From Ps and Pn we extract every element and merge to P to create a single dataset row\n",
    "    for i in Ps:\n",
    "        P[j]=i.astype(characterDataType)\n",
    "        j=j+1\n",
    "    \n",
    "    for i in Pn:\n",
    "        P[j]=i\n",
    "        j=j+1\n",
    "    \n",
    "    return P\n",
    "\n",
    "\n",
    "#computes the count of a single  atributes composed of both (numerical , character) attributes\n",
    "result=count([['71.126.222.64','b','c',57272.0, 80.0 ,1.0 ,1.0, 52.0, 5840.0],['71.126.222.64','x','c',1,7,1,2,4,5],\n",
    "              ['192.168.0.1','b','q',1,3,9,1,6,4]])\n",
    "print(\"Result: \",result)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Why preprocessing is done in this dataset?\n",
    "Ans: A single row of data both consist of both numerical and character data.First we assume every data is as \n",
    "character data and convert it to string.Then,if we find some missing data which is represented as 'nan' by 'pandas'\n",
    "we replace the value with some minimal value.Here,we previously initialized a variable named \"generalizedData\" with\n",
    "some minimal value so it would not affect other dataset.It can be changed to zero or any other value while testing\n",
    "datasets."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "nSimilarity computes similarity of only numerical attributes of both provided datarow.\n",
    "We preserved the data value till similarity computation by using float datatype.After calcuation It would be easier\n",
    "to perform calcuation on the integer datatype so we lastly converted data type to integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 873,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity n:  52635\n"
     ]
    }
   ],
   "source": [
    "def nSimilarity(ei,ej):\n",
    "    \n",
    "    '''\n",
    "    Parameter:ei and ej are the two records in the collection of dataset\n",
    "    returns:Similarity based on classical euclidean distance\n",
    "    This calculates only the similarity of numerical attribute\n",
    "    ei and ej only contains the numerical attributes\n",
    "    '''\n",
    "        \n",
    "    characterAttributeEi,ei=numericalAndCharacterDivision(ei)\n",
    "    characterAttributeEj,ej=numericalAndCharacterDivision(ej)\n",
    "    \n",
    "    \n",
    "    #Preprocessing\n",
    "    #All the nan elements are converted to 0 for calculation purpose\n",
    "       \n",
    "    j=0\n",
    "    for i in range(ei.shape[0]):\n",
    "        try:\n",
    "            #print(ei[j].dtype)\n",
    "            if(ei[j].astype('str') == 'nan'):\n",
    "                ei[j]=generalizedData                        \n",
    "        except:\n",
    "            #ei[j]=generalizedData\n",
    "            pass\n",
    "        j=j+1\n",
    "        \n",
    "    #print(ei)\n",
    "    #return\n",
    "    \n",
    "    j=0\n",
    "    for i in range(ej.shape[0]):\n",
    "        try:\n",
    "            if(ej[j].astype('str')=='nan'):\n",
    "                ej[j]=generalizedData\n",
    "        except:\n",
    "            #ej[j]=generalizedData\n",
    "            pass\n",
    "        j=j+1\n",
    "        \n",
    "    #ei[ei == 'nan']=0\n",
    "    ei=ei.astype(np.float32)\n",
    "    \n",
    "    #ej[ej=='nan']=0\n",
    "    ej=ej.astype(np.float32)\n",
    "  \n",
    "    #calculates the euclidean distance of parameters\n",
    "    return np.sqrt(np.sum((ei-ej)**2)).astype(np.int32)\n",
    "    \n",
    "  \n",
    "resultDistance=nSimilarity(np.array(['71.126.222.64', '126.120.0.39', 'TCP' ,'35641', '150' ,'434', '10710' ,'63',\n",
    " '19473']),\n",
    "                           np.array(['71.126.222.64', '132.244.39.212' ,'TCP', 57259.0, 80.0 ,2751.0 ,35280.0, 52,\n",
    " 60632.0]))\n",
    "print(\"Similarity n: \",resultDistance)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pSimilarity calculates the similarity of character attributes of both data row provided.\n",
    "Here some calculation can be skipped and returned the known result after checking array equality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 874,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pSimilarity(ei,ej):\n",
    "    '''\n",
    "    Parameter:ei and ej are the two records in the collection of dataset\n",
    "    returns:Similarity based on the frequency of attributes\n",
    "    This function calculates the similarity of all atributes (that includes both numerical and character attributes)\n",
    "    '''\n",
    "     #needs the number of Hjk and Hik of ei and ej\n",
    "    #Lets suppose ei and ej as:(based on e)\n",
    "    #ei: 192.168.0.1,101.16.1.89,80,3764,45213,5,1,67,78\n",
    "    #ej: 192.168.0.2,102,17,6,73,21,4562,17234,4,1,32,56\n",
    "    #result=((9+9)/(9*9)*A)\n",
    "    #A=0 if ei=ej else A=1\n",
    "    \n",
    "    A=1\n",
    "    #computing A first\n",
    "    #if Hik==Hjk then A=0 else A=1\n",
    "    if np.array_equal(ei,ej):#checks if both ei and ej have same element is array\n",
    "        #A=0\n",
    "        return 0\n",
    "    else:\n",
    "        A=1\n",
    "        #pSimilarity=0.67\n",
    "    \n",
    "    characterAttributeEi,numericalAttributeEi=numericalAndCharacterDivision(ei)\n",
    "    characterAttributeEj,numericalAttributeEj=numericalAndCharacterDivision(ej)\n",
    "    \n",
    "    #This line computes for the number of character attributes\n",
    "    #pSimilarity=((characterAttributeEi.shape[0] + characterAttributeEj.shape[0])\n",
    "    #/(characterAttributeEi.shape[0] * characterAttributeEj.shape[0]))*A\n",
    "    \n",
    "    #Preprocessing   \n",
    "    #This line computes for the number of character attributes but doesnot count the empty data\n",
    "    #every row sometimes doesnot contain every element \n",
    "    #so this line excludes the empty element in the data row\n",
    "    numberOfEi=np.array(list(filter(None,characterAttributeEi))).shape[0]\n",
    "    numberOfEj=np.array(list(filter(None,characterAttributeEj))).shape[0]    \n",
    "    \n",
    "  \n",
    "    #This line computes for the number of character attributes if any number of list is zero (0)\n",
    "    if(numberOfEi==0 or numberOfEj==0):\n",
    "        numberOfEi=characterAttributeEi.shape[0]\n",
    "        numberOfEj=characterAttributeEj.shape[0]\n",
    "        return (( numberOfEi+numberOfEj )/(numberOfEi* numberOfEj))*A\n",
    "    \n",
    "    \n",
    "    pSimilarity=((numberOfEi + numberOfEj)/(numberOfEi * numberOfEj)) * A\n",
    "    \n",
    "    \n",
    "    return pSimilarity \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 875,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P Similarity:  0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "resultPSimilarity=pSimilarity(np.array([1,2,3,4,5,6,7,8,9]),np.array([9,56,3,4,5,6,7,8,8]))\n",
    "print(\"P Similarity: \",resultPSimilarity)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "similar function combines both the pSimilarity and nSimilarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 876,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def similar(ei,ej):\n",
    "    '''\n",
    "    Parameter:ei and ej are the two records in the collection of dataset\n",
    "    returns:Similarity based on the frequency of attributes and euclidean distance of numerical attributes\n",
    "    '''\n",
    "    #computes the similarity of both nSimilarity and pSimilarity\n",
    "    return nSimilarity(ei,ej)+pSimilarity(ei,ej)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 877,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar:  72.8333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/py/anaconda3/lib/python3.5/site-packages/numpy/core/numeric.py:2417: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  return bool(asarray(a1 == a2).all())\n"
     ]
    }
   ],
   "source": [
    "resultSimilar=similar(np.array(['',2,3,4,5,6,7,8,9]),np.array([1,2,3,40,5,6,70,8,9]))\n",
    "print(\"Similar: \",resultSimilar)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This block extracts all cluster from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 878,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getAllClusterCenter(dataset):\n",
    "    \n",
    "    '''\n",
    "    Parameter:Dataset contains the data rows which contains both the numerical and character attributes\n",
    "    Returns:All cluster count\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    #1.we sample the dataset into group i.e. break the dataset into numberOfSample Size provided\n",
    "    #2.we get the center data(m[i]) for each sample using count function\n",
    "    #we select the first element of the previous result as the first cluster(m) center\n",
    "    #therefore m1=m[1]\n",
    "    #For second cluster,we calcuate the Similarity of the selected first cluster(m) with every\n",
    "    #other center data(m[i]) \n",
    "    #From Similarity we select the maximum similar clusters. therefore,m2=max(m1,m[i]) where i>1\n",
    "    #return object as (m1,m2)\n",
    "    \n",
    "    #divides the dataset perfect based on the number of samples\n",
    "    #if the dataset cannot be divided into equal number of samples,we decrease the size of dataset\n",
    "    #to make equal number rows in samples    \n",
    "    numberOfRows=dataset.shape[0] #initial number of rows in dataset\n",
    "    while((numberOfRows % numberOfSamples)!=0):#calculating the modulus (reminder),decrease until reminder is zero\n",
    "        dataset=np.delete(dataset,1,axis=0) #deletes any 1 row from the dataset\n",
    "        numberOfRows=dataset.shape[0] #newly defined number of rows in dataset\n",
    "        \n",
    "        \n",
    "    #splits the modified dataset into equal number of samples (group)\n",
    "    samples=np.split(dataset,numberOfSamples,axis=0)\n",
    "    \n",
    "    #This consist of the collection of center from the samples\n",
    "    countCollection=np.empty([numberOfSamples,numberOfAttributes],dtype=characterDataType)\n",
    "       \n",
    "    j=0\n",
    "    for i in samples:\n",
    "        countCollection[j]=count(i) #performs the count operation\n",
    "        j=j+1\n",
    "        \n",
    "    return countCollection"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This code returns the first and second cluster from the samples.\n",
    "For first cluster some of the code can be tweaked to get different results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 879,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def search(dataset):\n",
    "    \n",
    "    '''\n",
    "    Parameter:Dataset contains the data rows which contains both the numerical and character attributes\n",
    "    Returns:Create two initial Cluster\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    \n",
    "    countCollection=getAllClusterCenter(dataset)\n",
    "       \n",
    "    #print(\"Count Collection: \",countCollection)\n",
    "    \n",
    "    firstCenter=countCollection[0] #derived from m=m1\n",
    "   \n",
    "    #As alternative:\n",
    "    \n",
    "    #m=center {m1,m2, m3……..ml} \n",
    "    #Above may mean to determine the center from the center collected from samples\n",
    "    #For that simply enable below first center lines and comment the above firstCenter lines:\n",
    "    #firstCenter=count(countCollection)\n",
    "    #print(\"First: \",firstCenter)\n",
    "    \n",
    "    #For second center we need to calcuate the similarity among the collected center i.e. count with first center\n",
    "    \n",
    "    #We need to select the max Similar from result;which means that we need to select the smallest value\n",
    "    # as similar is near to zero i.e. Similar(1,1)=0 where as Similar(1,99)=97 (may be not,but not equal to 0)\n",
    "    \n",
    "    #initial variable\n",
    "    minVariable=similar(firstCenter,countCollection[1]) \n",
    "    index=0\n",
    "    j=0\n",
    "    \n",
    "    for i in countCollection:\n",
    "        #Excluded the first center if they are same\n",
    "        if(np.array_equal(firstCenter,i)==False): \n",
    "            similarity=similar(firstCenter,i)\n",
    "            if(minVariable>similarity):\n",
    "                minVariable=similarity\n",
    "                index=j\n",
    "        j=j+1\n",
    "        \n",
    "    secondCenter=countCollection[index]\n",
    "       \n",
    "    return (firstCenter,secondCenter)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This block calculates the minSimilar between the Cluster C        \n",
    "minClusterSimilar=similar(allClusters[0],allClusters[1]) #initializing any first two cluster similar\n",
    "clusterIndex=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 880,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calculateMinSimilarityBetweenCluster(clusters):\n",
    "    minClusterSimilar=similar(clusters[0],clusters[1])\n",
    "    for k in range(clusters.shape[0]):\n",
    "        # This skips previously determined similars\n",
    "        p=k+1 \n",
    "        for p in range(p,clusters.shape[0]):\n",
    "            similarity=similar(clusters[k],clusters[p])\n",
    "            if(similarity>minClusterSimilar):\n",
    "                minClusterSimilar=similarity\n",
    "            #print(\"Min Cluster Similar: Ci: %d\"% k +\" Cp: %d \" % p +\" Sim: %d \" % minClusterSimilar)\n",
    "    return minClusterSimilar"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Although this below code has not been used at all.But it can be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 881,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def addCluster(clusterCollection,clusterToAdd):\n",
    "    #for addition of the new data as cluster\n",
    "    newCluserSize=clusterCollection.shape[0]+1 \n",
    "    newCluster=np.empty([newCluserSize,numberOfAttributes],dtype=characterDataType)\n",
    "    newIndex=0\n",
    "    #This loop just adds the previous elments to new cluster\n",
    "    #todo optimize this\n",
    "    for i in clusterCollection:\n",
    "        newCluster[newIndex]=i\n",
    "        newIndex=newIndex+1                    \n",
    "        \n",
    "    #This line adds the cluster to add to the new cluster\n",
    "    newCluster[newIndex]=clusterToAdd.astype(characterDataType)\n",
    "    return newCluster"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This block determines the label for cluster in the cluster collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 882,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def determineLabelForCluster(allCluster,firstCluster,secondCluster):\n",
    "    clusterLabelDict={}\n",
    "    \n",
    "    #initial cluster center\n",
    "    #firstCluster , secondCluster\n",
    "    \n",
    "    #Distance between two cluster is considered as similarity between the cluster\n",
    "    #print(\"Distance from a cluster center to vh: \",similar(vh,allCluster[0]))\n",
    "    distanceCollection=np.zeros([allCluster.shape[0],2])\n",
    "    maxDistanceCollection=np.zeros(allCluster.shape[0])    \n",
    "    \n",
    "    for (i,cluster) in enumerate(allCluster):\n",
    "        distanceCollection[i][0]=similar(firstCluster,allCluster[i])\n",
    "        distanceCollection[i][1]=similar(secondCluster,allCluster[i])\n",
    "        maxDistanceCollection[i]=max(distanceCollection[i][0],distanceCollection[i][1])\n",
    "        \n",
    "    averageDistance=sum(maxDistanceCollection)/maxDistanceCollection.shape[0]\n",
    "    \n",
    "    print(\"Distance After Processing: \",distanceCollection)\n",
    "    print(\"Max Distance: \",maxDistanceCollection)\n",
    "    print(\"Average Distance: \",averageDistance)\n",
    "    \n",
    "    for (i,cluster) in enumerate(allCluster):\n",
    "        #cluster is normal if max distance is less than average distance\n",
    "        if(maxDistanceCollection[i]<averageDistance):\n",
    "            #this is normal\n",
    "            clusterLabelDict[i]='no'\n",
    "            #print(\"normal\")\n",
    "        else:\n",
    "            #This is attack\n",
    "            clusterLabelDict[i]='yes'\n",
    "            #print(\"Attack\")\n",
    "        pass     \n",
    "    \n",
    "    \n",
    "    #needs some more operation for labelling the cluster\n",
    "    #currently just passing the cluster index as the label\n",
    "   \n",
    "    #for (index,ei) in enumerate(allCluster):\n",
    "        #clusterLabelDict[index]=index        \n",
    "            \n",
    "    return clusterLabelDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 883,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preprocessEi(ei):\n",
    "    for (j,ej) in enumerate(ei):            \n",
    "        try:\n",
    "            x=float(ej)\n",
    "            if(math.isnan(x)==True):\n",
    "                ei[j]=generalizedData\n",
    "        except:\n",
    "            pass\n",
    "    return ei"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This block is used for visualization of dataset"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This block:\n",
    "1.First search the two cluster of data.\n",
    "2.Calculate the minimum similarity of ei (from data set)  with all the cluster \n",
    "3.Calculate the minimum similarity among the clusters.\n",
    "4.If min(Sim(ei,Cj))>min(Sim(C)) add new cluster as ei\n",
    "    else merge ei to Cj."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 884,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center Created:  ['71.126.222.64' '40.112.51.8' 'TCP' 55289.0 443.0 1061.0 13531.0 52\n",
      " 34800.0]  With index:  2  with ei index:  3\n",
      "Center Created:  ['71.126.222.64' '254.229.252.232' 'TCP' 37972.0 80.0 4038.0 37725.0 52\n",
      " 64088.0]  With index:  3  with ei index:  5\n",
      "Center Created:  ['71.126.222.64' '244.196.126.1' 'TCP' 34925.0 80.0 2893.0 57677.0 40\n",
      " 131068.0]  With index:  4  with ei index:  36\n",
      "Center Created:  ['71.126.222.64' '202.54.0.151' 'TCP' 60171.0 80.0 7618.0 225808.0 40\n",
      " 131068.0]  With index:  5  with ei index:  426\n",
      "Distance After Processing:  [[      0.           13121.66666667]\n",
      " [  13121.66666667       0.        ]\n",
      " [  38948.66666667   31153.66666667]\n",
      " [  69317.66666667   67581.66666667]\n",
      " [ 137621.66666667  135936.66666667]\n",
      " [ 259144.66666667  256831.66666667]]\n",
      "Max Distance:  [  13121.66666667   13121.66666667   38948.66666667   69317.66666667\n",
      "  137621.66666667  259144.66666667]\n",
      "Average Distance:  88546.0\n",
      "number of cluster:  6\n"
     ]
    }
   ],
   "source": [
    "def algorithmHeuristicCluster():\n",
    "    firstCluster,secondCluster=search(sampleData)\n",
    "    #Here either all cluster center can be used or only two cluster during initialization\n",
    "    \n",
    "    #This selects all the clusters determined during the sampling for initialization\n",
    "    #allClusters=getAllClusterCenter(dataset)\n",
    "    \n",
    "    #This selects only two of the cluster for initialization\n",
    "    allClusters=np.array([firstCluster,secondCluster])\n",
    "    \n",
    "    #dictionary has a key and its value\n",
    "    #Here,key would be index of the ei and value would be the cluster index\n",
    "    dictionaryClusterEi={}\n",
    "    \n",
    "    numberOfClusterFormed=np.shape(allClusters)[0]\n",
    "    \n",
    "    for eiIndex,ei in enumerate(dataset):\n",
    "        #this block calculates the minSimilar between the ei and cj\n",
    "        \n",
    "        #*************\n",
    "        j=0\n",
    "        #initialValue which may be the greatest among the dataset\n",
    "        minSimilar=similar(ei,allClusters[0]) \n",
    "        clusterIndex=0\n",
    "        for cj in allClusters:\n",
    "            similarity=similar(ei,cj)\n",
    "            if(similarity<minSimilar):\n",
    "                minSimilar=similarity\n",
    "                clusterIndex=j\n",
    "            j=j+1\n",
    "            \n",
    "        #************\n",
    "        \n",
    "        #This block calculates the minSimilar between the Cluster C\n",
    "        \n",
    "        minClusterSimilar=calculateMinSimilarityBetweenCluster(allClusters)         \n",
    "        if(minSimilar>minClusterSimilar):\n",
    "            #merges the ei to cluster   \n",
    "            allClusters=np.concatenate((allClusters,np.array([ei],dtype=characterDataType)),axis=0) \n",
    "               \n",
    "            print(\"Center Created: \",ei,\" With index: \",allClusters.shape[0]-1,\" with ei index: \",eiIndex)\n",
    "            #Since ei is the new cluster and it is concatenated as last index\n",
    "            dictionaryClusterEi[eiIndex]=allClusters.shape[0]-1            \n",
    "            \n",
    "            #number of cluster is not needed for operation its just for information\n",
    "            numberOfClusterFormed=numberOfClusterFormed+1\n",
    "                    \n",
    "        else:\n",
    "            #holds the index for cluster for every ei\n",
    "            dictionaryClusterEi[eiIndex]=clusterIndex\n",
    "    \n",
    "        \n",
    "    #creates the label to every cluster index\n",
    "    clusterLabel=determineLabelForCluster(allClusters,firstCluster,secondCluster)\n",
    "        \n",
    "    #new data set with the size of previous dataset and cluster label as [n,10]\n",
    "    #   where n = number of dataset or row,10 is number of column\n",
    "    newDataset=np.zeros([dataset.shape[0],10],dtype=characterDataType)\n",
    "    \n",
    "    #appends the cluster index to every ei in data record\n",
    "    for (index,ei) in enumerate(dataset):\n",
    "        ei=np.append(ei,clusterLabel[dictionaryClusterEi[index]])\n",
    "        \n",
    "        #Preprocessing removes nan from the ei and replaces with the value of generalizedData set above\n",
    "        #newDataset[index]=ei.astype('str')\n",
    "        newDataset[index]=preprocessEi(ei)\n",
    "        \n",
    "        #To see the data that is being saved disable the comment\n",
    "        #print(\"ei: \",ei,\" Cluster Label: \",clusterLabel[dictionaryClusterEi[index]])\n",
    "        \n",
    "        \n",
    "    #print(\"Dictionary (ei,Cluster Index): \",dictionaryClusterEi)\n",
    "    #print(\"Dictionary (clusterIndex,Class Label): \",clusterLabel)\n",
    "    \n",
    "    \n",
    "    #This creates a new file containing ei and the cluster label\n",
    "    dataFrame=pd.DataFrame(newDataset)\n",
    "    dataFrame.to_csv(\"test.csv\",header=None)\n",
    "    \n",
    "    #This block is test for visualization of data\n",
    "        \n",
    "    return numberOfClusterFormed\n",
    "\n",
    "#for i in range(0,10):\n",
    "print(\"number of cluster: \",algorithmHeuristicCluster())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
